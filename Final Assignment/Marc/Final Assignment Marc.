###FINAL ASSIGNMENT

#Marc Verwoert s4718801
#Vera Tukke s4766970

library(ggplot2)
library(car)
library(pastecs)
library(effects)
library(gvlma)
library(lme4)
library(lmerTest)
library(MuMIn)

#Some general tips:

#Remember you need plots of your effects. If you have many predictors, there can be too many graphs for use of the allEffects() function. In this case, it is better to plot the effects individually. 

#If your interaction is between 2 numerical predictors, it's better to use the contour/elevation plots we went over a couple of times.

#When checking for multicollinearity, note that vif values for interactions and single predictors that participate in interactions can be quite high. This is nothing to worry about. Single predictors that do not participate in interactions, however, should have vif values below 10.

#If you violate an assumption, use a different method if we've learned about one. However, if not, just note the assumption violation when you do the reporting and what this means for the interpretation of the results.

#In addition to the standard reporting of the results, please give a brief discussion of the findings. For example, if there is a significant interaction, talk about why this might be. What does the interaction actually mean logically? It's ok to speculate a bit here, since this is a statistics course and not a course about any one particular field.

#You'll have to load the necessary libraries! I do not provide those lines of code here!

#Also, when using drop1(), you must use the Chisq test instead of the F test when you have created a model using random intercepts!

#It may be necessary to remove rows that contain missing data!

###################
#####Swimming Data
###################

#datafile: swimming.csv

#In this study, the times it took for different participants to swim a lap in a swimming pool were measured. Swimmers varied in terms of which end of the pool they started from (End variable), whether they wore goggles, whether they wore a shirt, and whether they used flippers. Perform an analysis and consider up to 2-way interactions between predictors. 

swimmingData<- read.table(file.choose(), sep="\t", comment.char="", quote="", header=T)

#Outcome variable is time it took to swim a lap.

#We have added an interaction between two predictors using the ":"
swimming_model<- lm(Time ~ End + Shirt + Goggles + Flippers + End:Shirt + End:Goggles + End:Flippers+ Shirt:Goggles + Shirt:Flippers + Goggles:Flippers, data=swimmingData)

summary(swimming_model)

drop1(swimming_model, test="F")
#The interaction between everything except for Goggles: Flippers is (highly) non-significant and have a lower AIC. So, we drop all those interactions.

new_swimming_model2 <- swimming_model<- lm(Time ~ End + Shirt + Goggles + Flippers + Goggles:Flippers, data=swimmingData)
drop1(new_swimming_model2, test="F")
#End was not-significant and dropped the AIC value, so we can drop this predictor.

new_swimming_model <- swimming_model<- lm(Time ~ Shirt + Goggles + Flippers + Goggles:Flippers, data=swimmingData)
drop1(new_swimming_model) # Nothing to drop anymore.

#Checking Assumptions:
#Auto-correlation
durbinWatsonTest(new_swimming_model)
#Statistic=  1.886885 , very close to 2, so assumption has certainly been met, so probably not correlated. P-value of Value .636 confirms this, it is bigger than .05, and therefore non-significant. The value however is less than 2, so indicates a positive correlation. So this assumption has been met, and thus the residuals are uncorrelated. This doesn't indicates some amount of autocorrelation.

#Homoscedasticity normality, and high-influence points:

#No severe multicollinearity
vif(new_swimming_model)          
#No concering VIF values. All around 1.

#Residuals:
par(mfrow=c(2,2))
par(mar=c(4,4,4,4))
plot (new_swimming_model)

#Linearity: There no sort of curve in the (first) graph, this indicates that the data hasn't violated the assumption of linearity.

#Normal distributed: Almost all of the datapoints fall kinda on the line, so it's probably normal.
#Homoscedasticity: This assumption does hold, the datapoints don't really funnel out. Also it looks like a random array of dots evenly dispersed around zero. So this doesn't indicate variance across residuals and thus doesn't indicate heteroscedasticity. 

#Leverage points:
dim(swimmingData)
averageSwimming<- (1+4)/70; averageSwimming #=  0.01 #(k+1)/n

swimmingHat <- hatvalues(new_swimming_model) 
swimmingTable<-swimmingHat > 2 *  averageSwimming | swimmingHat > 3*   averageSwimming; swimmingTable
table(swimmingTable)
#None of the values are 2 or 3 times bigger than the ave. value, and thus none affect the data points positioning.
#Also if you look at the graph: we have not really datapoints with extreme x-values, so we don't have leverage points, so we don't have cause for concern. 

#Influential points:
cookSwimming<-cooks.distance(new_swimming_model)
cookSwimmingTable<-cookSwimming > 1; cookSwimmingTable
table(cookSwimmingTable)
#Non of the values are bigger than 1. So, there are no influential points, and thus no outliers, that may affect the slope of the regression line.
#If you also look at the graph non of the points fall in the cook's distance and thus influence the cloud of the dots. The cloud of datapoints is pretty close to each other, and we don't have one point which affects the regression line highly.

plot(allEffects(new_swimming_model))
##When a shirt was worn, the amount of time went up. End: north was less time costly than south. When goggles were no, and flippers were no, the time was alot higher, then when goggles was yes while flippers were no. When however, Flippers were yes, and goggles were no, the time needed for swimming was suprisingly slightly better than when flippers was yes and goggles was yes. So the best was when either: flippers  and no goggles were worn and when no flippers were worn but goggles were worn.

#Means for each combination of factor levels
by(swimmingData$Time, list(swimmingData$Shirt,swimmingData$Goggles, swimmingData$Flippers, swimmingData$End), stat.desc)
# Shows, that the means are clearly different. The means for time have lower values, when goggles and flippers are worn then when they aren't.

#We're now going to do a ANOVA, to see if the difference is significant.

Anova(new_swimming_model, type="III")
summary(new_swimming_model)

#The final model formula was Time~ End + Shirt + Flippers + Goggles+ Goggles:Flippers. The main effect End was not sigificant (p>0.5), but the main effects flippers, shirt, goggles were (p<0.5) significant. The interaction between flippers and goggles was significant (p<0.05). Specifically, flippers showed a significant decrease in time needed for laps as the swimmers moved from no flippers to yes-flippers. When flippers and goggles were put together, compared to no flippers and goggles, time needed for laps increased when those were both no. The model was highly significant overall (F(4,65)=14.11, p<0.001) with a large degree of explained variance (Multiple R-squared:  0.4648,	Adjusted R-squared:  0.4319). All regression coefficients, as well as their standard errors, t-scores, and p-values, are provided in the appendix. Checking of model assumptions revealed no problems.

#Appendix:

#(Intercept)             22.1894     0.5945  37.325  < 2e-16 ***
#  Shirtyes                 1.6635     0.5712   2.912 0.004910 ** 
#  Gogglesyes              -2.8177     0.7374  -3.821 0.000300 ***
#  Flippersyes             -5.0908     0.8021  -6.347 2.43e-08 ***
#  Gogglesyes:Flippersyes   4.2029     1.1424   3.679 0.000477 ***
#  ---

#In addition to the standard reporting of the results, please give a brief discussion of the findings. For example, if there is a significant interaction, talk about why this might be. What does the interaction actually mean logically? It's ok to speculate a bit here, since this is a statistics course and not a course about any one particular field.

#Like we saw in one of the plots we made earlier, If flippers and goggles are worn at the same time, there is a big chance that the swimmer will swim slower than when he/she does not do that. The amount of time needed was lower when either flippers or goggles were used. Especially when flippers the time was a lot faster, even when goggles were not worn, while when they both were worn the time was slower, which gives an indication towards the conclusion that especially flippers are very important wrt to better swimmer times. Swimming with a shirt has a a result a slower time. Also end south takes more time than end north. 

##########################
##########Reaction Times
##########################

#datafile: reaction_times.csv

#In this study, participants had to make a decision on whether a sequence of letters presented to them on a computer screen was a word or not. How long it took for them to make this decision was recorded (their reaction time). This is called a lexical decision task. In this kind of task, many different factors can influence reaction times, including how frequent the word is, how familiar the work is, and how "imageable" it is--that is, how easy it is to imagine a picture of the word's meaning. Investigate whether, for this data set, this is true. Consider up to the 3-way interaction. Note that reaction times are averages from across participants, so although this was originally a within-subject experiment, participant-level variance is not shared across data points.

RTs <- read.table(file.choose(), header=T, sep="\t", row.names=1)
head(RTs)

#In this data, we will predict reaction times to word stimuli on the basis of how frequent, familiar and imageabile the words are; thus more frequent, imageabile and familiar words should be reacted to faster.

#Check levels; hi is ref level since it is alphabetically first
levels(RTs$FAMILIARITY)

#Change default level to lo
RTs$FAMILIARITY <- relevel(RTs$FAMILIARITY, "lo"); levels(RTs$FAMILIARITY)
rt_model <- lm(RT ~ FREQUENCY + FAMILIARITY + IMAGEABILITY + FAMILIARITY:FREQUENCY+ FAMILIARITY: IMAGEABILITY +FREQUENCY:IMAGEABILITY, data=RTs)

#Model selection:
drop1(rt_model, test="F")
#All of the interactions are highly non-significant and have a lower AIC. So, we drop all these interactions.

new_rt_model <- lm(RT ~ FREQUENCY + FAMILIARITY + IMAGEABILITY, data=RTs)
drop1(new_rt_model, test="F")
#This shows that IMAGEABILITY is highly non-significant and has a lower AIC, so we drop this predictor.
rt_model_planned_2 <- lm(RT ~ FREQUENCY + FAMILIARITY, data=RTs)
drop1(rt_model_planned_2, test="F")#Nothing to drop anymore, all significant and higher AIC.

plot(allEffects(rt_model_planned_2))
#The plots show that: when frequency goes up, the reaction times go indeed down. And when frequency goes down, reaction times go up.
#The plots show that: when familiarity is high, reaction times are lower, then when familiarity is low.

#Checking Assumptions:
#Auto-correlation
durbinWatsonTest(rt_model_planned_2)
#Statistic=    2.429317, quite close to 2, so assumption has proably been met, so probably not correlated. P-value of Value .147 confirms this, it is bigger than .05, and therefore non-significant. The value however is more than 2, so indicates a negative correlation. So this assumption has been met, and thus the residuals are uncorrelated. This doesn't indicates some amount of autocorrelation.

#Homoscedasticity normality, and high influence points

#No severe multicollinearity
vif(rt_model_planned_2)          
#No concering VIF values. All around 1.

#Residuals:
par(mfrow=c(2,2))
par(mar=c(4,4,4,4))
plot (rt_model_planned_2)

#Linearity: There is a bit of a curve in the (first) graph, but nothing to outrageous. This indicates that the data hasn't violated the assumption of linearity.

#Normal distributed: almost all of the datapoints fall kinda on the line, so it's probably normal.
#Homoscedasticity: this assumption does hold, the datapoints don't really funnel out. Also it looks like a random array of dots evenly dispersed around zero. So this doesn't indicate variance across residuals and thus doesn't indicate heteroscedasticity. 

#Leverage points:
dim(RTs)
averagert<- (1+2)/48; averagert #=  0.01 #(k+1)/n

rtHat <- hatvalues(rt_model_planned_2) 
rtTable<-rtHat > 2 *  averagert | rtHat > 3*   averagert; rtTable
table(rtTable)
#Some of the values are 2 or 3 times bigger than the ave. value, and thus some affect the data points positioning.
#Also if you look at the graph: we have  some  datapoints with extreme x-values, so we do have leverage points, so we have some cause for concern. 

#Influential points:
cookrt<-cooks.distance(rt_model_planned_2)
cookrtTable<-cookrt > 1; cookrtTable
table(cookrtTable)
#None of the values are bigger than 1. So, there are no influential points, and thus no outliers, that may affect the slope of the regression line.
#If you also look at the graph non of the points fall in the cook's distance and thus influence the cloud of the dots. The cloud of datapoints is pretty close to each other, and we don't have one point which affects the regression line highly.

by(RTs$RT, list(RTs$FREQUENCY,RTs$FAMILIARITY), stat.desc)
# Shows, that the means are clearly different. The means RT are highe when freq is high and when fam is high.

#And we can get statistics for the predictors:
Anova(rt_model_planned_2, type="III")

summary(rt_model_planned_2)

#Reporting: The final model's formula was RT ~ FREQUENCY + FAMILIARITY. The overall effect of FAMILIARITY was significant (p < 0.05). There was also a significant effect of FREQUENCY (p<0.05); as Frequency increased, reaction times decreased. Finally, the model was significant overall (F(2,45)=5.671, p<0.05) and explained a small amount of variance (Multiple R-squared:  0.2013,	Adjusted R-squared:  0.1658 . All regression coefficients, as well as their standard errors, t-scores, and p-values, are provided in the appendix. Checking of model assumptions revealed no problems.

#Appendix: 
#  Coefficients:
#  Estimate Std. Error t value Pr(>|t|)    
#  (Intercept)    665.794     16.939  39.306   <2e-16 ***
#  FREQUENCY      -10.386      5.101  -2.036   0.0477 *  
#  FAMILIARITYhi  -38.738     17.153  -2.258   0.0288 *  
#  ---
#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#Residual standard error: 41.21 on 45 degrees of freedom
#Multiple R-squared:  0.2013,	Adjusted R-squared:  0.1658 
#F-statistic: 5.671 on 2 and 45 DF,  p-value: 0.006362

#In addition to the standard reporting of the results, please give a brief discussion of the findings. For example, if there is a significant interaction, talk about why this might be. What does the interaction actually mean logically? It's ok to speculate a bit here, since this is a statistics course and not a course about any one particular field.

# The findings showed that the 3 predictors hadn't had any meaningful interaction between them, since they could be dropped. So, apparantely the 3 predictors didn't effect each other in a meaningful way. Imageability could be dropped as a predictor, so this didn't matter wrt to reaction time, thus an more imageabile word, didn't mean an faster reaction time.  However, as words become more familiar, the rt increased. This indicates that the bigger the association a person had with a word, the reaction times were faster. Frequency did contribute significantly to a lower RT, thus apparently the more frequent a word appeared, the lower the raction time.

#########################
#########Housing Data
#########################

#Datafile: housing_data.csv

#In this study, researchers collected data on crime rates for different city blocks in and around Boston in the U.S. You are interested in determining which variables predict crime rate. You are particularly interested in how economic status and ethnic diversity interact with each other as well as how each one interacts with the other predictors. Perform a proper analysis. The column information is below. (Note: because towns are repeated, we are actually violating the independence assumption. Normally, we would want to assign random intercepts to each town, but because we do not have a "town" variable, this is difficult...so we'll just ignore this here :-)


#1. CRIM      per capita crime rate by block
#2. INDUS     proportion of non-retail business acres per town
#3. CHAS      whether property is next to the Charles River or not
#4. NOX       nitric oxides concentration (parts per 10 million)
#5. RM        average number of rooms per dwelling
#6. AGE       proportion of owner-occupied units built prior to 1940
#7. DIS       weighted distances to five Boston employment centres
#8. RAD       index of accessibility to radial highways
#9. PTRATIO   pupil-teacher ratio by town
#10. B        measure of ethnic diversity
#11. LSTAT    % lower economic status of the population
#12. MEDV     Median value of owner-occupied homes in $1000's


read_housing<- read.table(file.choose(), sep="\t", comment.char="", quote="", header=T)

#And we make a lm model again.
houseModel <- lm(CRIM ~	INDUS + CHAS + NOX + RM + AGE +	DIS + RAD + PTRATIO + B	+ LSTAT + 	MEDV + B:LSTAT + B:INDUS + B:CHAS + B:NOX + B:RM + B:AGE + B:DIS + B:RAD + B:PTRATIO + B:MEDV + LSTAT:INDUS + LSTAT:CHAS + LSTAT:NOX +LSTAT:RM +LSTAT:AGE + LSTAT:DIS + LSTAT:RAD +LSTAT:PTRATIO + LSTAT:MEDV, data=read_housing)
summary(houseModel)

drop1(houseModel, test = "F")

#Then we look at if the predictors are significant.We drop the ones that are not highly significant.
#So we keep: B:LSTAT RM:B AGE:B DIS:B RAD:B B:MEDV AGE:LSTAT DIS:LSTAT RAD:LSTAT LSTAT:MEDV
newHouseModel <- lm(CRIM ~	INDUS + CHAS + NOX + RM + AGE +	DIS + RAD + PTRATIO + B	+ LSTAT + 	MEDV + B:LSTAT + B:LSTAT + RM:B + AGE:B + DIS:B + RAD:B + B:MEDV + AGE:LSTAT + DIS:LSTAT + RAD:LSTAT + LSTAT:MEDV, data=read_housing)
summary(newHouseModel)

drop1(newHouseModel, test = "F")

newSecondHouseModel <- lm(CRIM ~	NOX + RM + AGE +	DIS + RAD + B	+ LSTAT + 	MEDV + B:LSTAT + B:LSTAT + RM:B + AGE:B + DIS:B + AGE:LSTAT + RAD:LSTAT + LSTAT:MEDV, data=read_housing)
drop1(newSecondHouseModel, test="F")#Now they all are significant, and higher AIC. So, we don't drop any more predictors or interactions.
summary(newSecondHouseModel)

#Assumptions:
#Residuals:
par(mfrow =c(2,2))
par(mar=c(3,3,3,3))
plot (newSecondHouseModel)

#Checking Assumptions:
#Auto-correlation
durbinWatsonTest(newSecondHouseModel)
#Statistic= 1.651089, not quite close to 2, so assumption hasn't probably been met, so probably correlated. P-value of Value .018 confirms this, it is smaller than .05, and therefore significant. The value however is less than 2, so indicates a positive correlation. So this assumption hasn't been met, and thus the residuals are correlated. This does indicate some amount of autocorrelation, but since the statistic value is pretty close to 2, and the p-value isn't that far away from 0.05, we have a bit of autocorrelation, which isn't that bad, especially with so many predictors.

#Homoscedasticity normality, and high influence points

#When checking for multicollinearity, note that vif values for interactions and single predictors that participate in interactions can be quite high. This is nothing to worry about. Single predictors that do not participate in interactions, however, should have vif values below 10.

vif(newSecondHouseModel)          
#Predictors themself have also some incrediblly high values, which does indicate some severe multicollinearity. So this assumption has been broken, and thus some predictors are strongly correlated.

#Residuals:
par(mfrow=c(2,2))
par(mar=c(4,4,4,4))
plot (newSecondHouseModel)

#Linearity: There not really a sort of curve in the (first) graph, this indicates that the data hasn't violated the assumption of linearity.
#Normal distributed: alot of the datapoints don't fall kinda nicely on the line, so it's isn't normal. But since we have more than 506 datapoints, we can assume it's normal!
#Homoscedasticity: this assumption doesn't hold, the datapoints do really funnel out. Also it doesn't reallylooks like a random array of dots evenly dispersed around zero. So this does indicate variance across residuals and thus does indicate heteroscedasticity. 

#Leverage points:
dim(housingData)
averagehouse<- (1+11)/506; averagert #=  0.01 #(k+1)/n

houseHat <- hatvalues(newSecondHouseModel) 
houseTable<-houseHat > 2 *  averagehouse | houseHat > 3*   averagehouse; houseTable
table(houseTable)
#80 of the values are 2 or 3 times bigger than the ave. value, and thus alot of the datapoints affect the data points positioning.
#Also if you look at the graph: we have alot of datapoints with extreme x-values, so we do have leverage points, so we do have cause for concern. 

#Influential points:
cookhouse<-cooks.distance(newSecondHouseModel)
cookhouseTable<-cookhouse > 1; cookhouseTable
table(cookhouseTable)
#None of the values are bigger than 1. So, there are no influential points, and thus no outliers, that may affect the slope of the regression line.
#If you also look at the graph non of the points fall in the cook's distance and thus influence the cloud of the dots. The cloud of datapoints is pretty close to each other, and we don't have one point which affects the regression line highly.

#For main effects, you can do this by using plot(Effect("name of effect", name_of_model_object))
#For interactions, you can use plot(Effect(c("name of effect 1","name of effect 2"), name_of_model_object))
plot(allEffects(newSecondHouseModel))
#Since there are alot of plots and interactions, we are giving a general overview. 
#It seems that in general when B is high and LSTAT is low, crime rate is high. The interaction for those two main effects with other main effects, can just slightly variate the rates by increasing them or decreasing them by bits, but they don't have a very big influence.

summary(newSecondHouseModel)
Anova(newSecondHouseModel, type="III")

#Sum Sq  Df F value    Pr(>F)    
#(Intercept)   346.1   1 10.6891 0.0011535 ** 
#  NOX           146.7   1  4.5325 0.0337544 *  
#  RM            800.4   1 24.7231 9.169e-07 ***
#  AGE           570.3   1 17.6137 3.214e-05 ***
#  DIS           372.0   1 11.4916 0.0007556 ***
#  RAD           104.4   1  3.2254 0.0731195 .  
#B             432.9   1 13.3700 0.0002833 ***
#  LSTAT          11.8   1  0.3647 0.5461595    
#MEDV           29.5   1  0.9122 0.3400061    
#B:LSTAT      1092.0   1 33.7305 1.140e-08 ***
#  RM:B          902.0   1 27.8610 1.962e-07 ***
#  AGE:B         539.9   1 16.6766 5.176e-05 ***
#  DIS:B         327.0   1 10.1004 0.0015760 ** 
 # AGE:LSTAT     160.8   1  4.9666 0.0262941 *  
#RAD:LSTAT     293.0   1  9.0498 0.0027624 ** 
 # LSTAT:MEDV   1379.0   1 42.5944 1.684e-10 ***
  #Residuals   15864.0 490                      

  #Reporting: The final model's formula was CRIM ~	NOX + RM + AGE +	DIS + RAD + B	+ LSTAT + 	MEDV + B:LSTAT + B:LSTAT + RM:B + AGE:B + DIS:B + AGE:LSTAT + RAD:LSTAT + LSTAT:MEDV. The above list of main effects and the two-way interactions were significant.
  #The model was highly significant overall (F(15,490)=44.27, p<0.001) and achieved a high variance explanation (Multiple R-squared:  0.5754,	Adjusted R-squared:  0.5624 ). All regression coefficients, as well as their standard errors, t-scores, and p-values, are provided in the appendix, and checking of model assumptions revealed problems with autocorrelation, multicoliniearity and leverage points.

#Estimate Std. Error t value Pr(>|t|)    
#(Intercept) 54.8229912 16.7684285   3.269 0.001153 ** 
#  NOX         -8.9699773  4.2132848  -2.129 0.033754 *  
#  RM          -9.0204755  1.8141688  -4.972 9.17e-07 ***
#  AGE          0.5199845  0.1238983   4.197 3.21e-05 ***
#  DIS         -7.4956527  2.2111571  -3.390 0.000756 ***
#  RAD          0.1748103  0.0973362   1.796 0.073120 .  
#B           -0.1616095  0.0441979  -3.656 0.000283 ***
#  LSTAT       -0.1821863  0.3016603  -0.604 0.546160    
#MEDV         0.0606482  0.0635003   0.955 0.340006    
#B:LSTAT      0.0032445  0.0005586   5.808 1.14e-08 ***
#  RM:B         0.0276358  0.0052357   5.278 1.96e-07 ***
#  AGE:B       -0.0012586  0.0003082  -4.084 5.18e-05 ***
# DIS:B        0.0182488  0.0057420   3.178 0.001576 ** 
#  AGE:LSTAT   -0.0047618  0.0021367  -2.229 0.026294 *  
#  RAD:LSTAT    0.0170455  0.0056662   3.008 0.002762 ** 
 # LSTAT:MEDV  -0.0404505  0.0061979  -6.526 1.68e-10 ***

#In addition to the standard reporting of the results, please give a brief discussion of the findings. For example, if there is a significant interaction, talk about why this might be. What does the interaction actually mean logically? It's ok to speculate a bit here, since this is a statistics course and not a course about any one particular field.

#Based on the results above, we can conclude that INDUS, RM, AGE, DIS, RAD,B and LSTAT significantly predict crimerate. Which are alot of predictors which have to do with the economy/welfare of the town. Thus, apparently poverty in the population causes higher crime rates, than population which are economically richer. The interaction between B:LSTAT was also highly significant, and when you looked at the plot, we could see that when ethnic diversity was higher in a town, the economic status was also lower, resulting in the conclusion that apparently the more ethnic diverse people there are, the lower the economic status and thus the higher the crimerate. All of the interactions with either B or LSTAT were significant, which shows that those two predicors apparently highly influence the rest of the predictors, and thus indicating that those two are quite important with respect to predicting crimerate. It also makes sense that the interaction between B and the other predictors are significant, because most of the other predictors are economically related, and apparently ecnomcially related predictors are related with ethnic diversity. And ethnic diversity also seems to influence the crimerate in a town. The higher the diversity, the higher the crimerate.

#Also since alot of the predictors are of the same kind, e.g economically it makes sense that some assumptions were broken. Because, since they are of the same kind, they correlate alot.


###############################
#########Idiom Experiment
###############################

#Datafile: idiom_lexical_decision.csv

#In this experiment, participants were shown Dutch idioms such as "de koe bij de hoorns vatten" (grab the cow by the horns), which hase a figurative meaning of "to take control of a situation." After each idiom, they were shown either a word or a nonword and had to decide whether it was a word or not. The words were related either to the figurative meaning ("Fig" Condition) or to the literal meaning of the last word ("hoorns", i.e. the "Lit" Condition). The experimenter measured how much time, in milliseconds, it took for the participants to make the decision (this is called a lexical decision task), and then she took the log of these reaction times. If reading the idiom activated its figurative meaning in the mind of the subject, then they will be quicker to decide that a figuratively related word ("Fig") is indeed a word. If reading the idioms activated its literal meaning, then the subject will be quicker to make a decision on a literally related words. 

#Other factors might affect reaction times as well. These include: 
#the frequency of the word, 
#the literal plausibility (LP) of the idiom (how easy is it to interpret literally, e.g., "staan voor aap" = "stand for ape" is not literally plausible!)
#the transparency of the idiom (how related are the figurative and literal meanings, e.g., "tegen de lamp lopen" = "walk into the lamp" is not very related to figurative meaning "get caught")
#How well the subject knows the idiom (Knowledge)

#Your job is to conduct an analysis of this data and report your results. Consider all 2-way interactions. 

#Note: there are multiple data points per subject and per idiom, so both of the variables cause a violation of the independence assumption of regression. You will thus need to give both of these variables random intercepts. (you can just add a second random intercept term to your model equation like you add a first one)

#Note: You only need to check the assumptions of homoscedasticity and normally distributed residuals. Plotting the model will only give you the first diagnostic plot, which you can use to check homoscedasticity. To check for normal residuals, you can first get the residuals by passing the model object to the residuals() function, and then plotting the output.


idiomData<- read.table(file.choose(), sep="\t", comment.char="", quote="", header=T)

#Random Intercepts

idiom_model<-lm(log_RTs~Condition+Freqs+Knowledge+LP+Transparency+ Condition:Freqs+ Condition:Knowledge + Condition: LP + Condition:Transparency + Freqs:Knowledge+ Freqs:LP +Freqs:LP+ Freqs:Transparency +Knowledge:LP + LP:Transparency, data=idiomData)
#drop1(idiom_model,  test="F")
summary(idiom_model)

#Problem: violation of independence assumption! Each subject and idiom provides multiple data points.
#To account for this, we can give each subject their own intercept (and thus their own regression line).

#Run a mixed model with random intercepts
m2 <- lmer(log_RTs~Condition+Freqs+Knowledge+LP+Transparency+ Condition:Freqs+ Condition:Knowledge + Condition: LP + Condition:Transparency + Freqs:Knowledge+ Freqs:LP +Freqs:LP+ Freqs:Transparency +Knowledge:LP + LP:Transparency + 
             (1|Subject) + (1|Idiom), data=idiomData);m2

summary(m2)
# Random intercepts for each participant:
coef(m2)

drop1(m2, test="Chisq")
# None of the interactions are significant and have a higher AIC, so we can drop all those interactions. Except for Condition:Knowledge, this one has a higher AIC and is almost significant.
m3 <- lmer(log_RTs~Condition+Freqs+Knowledge+LP+ Condition:Knowledge +Transparency + (1|Idiom)
           + (1|Subject), data=idiomData);m3
summary(m3)

#Calculate r2
r.squaredGLMM(m3) #R2m (marginal r squared) is the one we want--it tells us how much variance we would expect to be explained in the population)
#0.02403555= 2.4%

#Assumptions:

#Residuals:
par(mfrow=c(2,2))
par(mar=c(4,4,4,4))
plot (m3)

#Homoscedasticity: this assumption does hold, the datapoints don't really funnel out. Also it does look like a random array of dots evenly dispersed around zero. So this doesn't indicate variance across residuals and thus doesn't indicate heteroscedasticity. 

#m3Norm<-qplot(sample=residuals(m3), stat="qq"); m3Norm
qqnorm(residuals(m3))
#Normal distributed: the plot has pretty normal curve. Alot of the datapoints would fall kinda nicely on a linear line, so it's probably normal distributed.

plot(allEffects(m3))
# The 1th graph showed:Apparantly the condition had very little influence on the RTs. But the RTs from where the last words were in literal conditions were slightly lower.
#The 2th graph showed: When words were more frequent, the RTs went down.
#The 3th graph showed: As knowledge went up, it seemed that Rts went down.
#The 4th graph showed: As words become more LP and thus easier to interpret, the RTs were lower.
#The 5th graph showed: Then when Transparancy went up, RTs also went up. When the relation between the figurative and literal meanings went up, RTs went up.
#The 6th graph showed:that when condition was figurative and knowledge was high, that RTs went down. And that when condition was lit, that a high or low knowledge didn't really matter.

Anova(m3, type="III")
summary(m3)

#The final model's formula was log_RTs~Condition+Freqs+Knowledge+LP+Transparency. The overall effect of knowledge and freqs were significant p<0.05.None of the interactions were significant. However the interaction between Condition:Knowledge was marginally significant. The model was significant overall (F(14,2051)=5.078, p<0.001) and explained small amount of variance (mult. R2=0.0240).All regression coefficients, as well as their standard errors, t-scores, and p-values, are provided in the appendix. Checking of model assumptions revealed no problems.

#In addition to the standard reporting of the results, please give a brief discussion of the findings. For example, if there is a significant interaction, talk about why this might be. What does the interaction actually mean logically? It's ok to speculate a bit here, since this is a statistics course and not a course about any one particular field.

#There were no significant interacions. However the interaction between Condition:Knowledge was marginally significant, indicating that there was a slight relation between the two, that when the condition was fig, reaction times where still low when the participant had high knowledge of the word. And that knowledge didn't really matter when the conidition was lit. Apparantly also, the only signifcant predictors were knowledege and freqs wrt to reaction times. So, the more a idiom appeared resulted in faster reactions times. And when an idiom was more known to the participant, this also resulted in faster reaction times.

#Appendix:
#(Intercept)   6.529e+00  8.316e-02  2.730e+01  78.510  < 2e-16 ***
#  ConditionLit -3.921e-03  8.964e-03  1.769e+03  -0.437   0.6619    
#Freqs        -6.740e-02  1.173e-02  2.606e+02  -5.747 2.53e-08 ***
#  Knowledge    -6.857e-03  3.023e-03  1.767e+03  -2.268   0.0234 *  
#  LP           -8.874e-03  8.975e-03  2.130e+01  -0.989   0.3339    
# Transparency  1.806e-02  2.309e-02  2.260e+01   0.782   0.4422 


#########################
#########Attributions
#########################

#Datafile: clause_order_data.csv

#In this experiment, participants read stories about different people and then indicate how favorable they feel about these people (the normV variable). The last sentence of a story is something like "Jay climbed through a window and stole the jewels, police said." This sentence can either be positive or negative (in the Exp column, "Pos" or "Neg"). Furthermore, the story itself can overall be a mixture of positive and negative actions (storyType=mixed)) or it can agree with the tone of the final sentence (storyType=accordant); that is, a positive story can go with a positive final sentence and a negative story can go with a negative final sentence. Finally, the "attribution phrase" (i.e., "police said") in the sentence can come either at the beginning of the sentence or the end: "Jay climbed through a window and stole the jewels, police said" versus "Police said that Jay climbed through the window and stole the jewels." Attribution phrases are known to weaken statements when placed first. Therefore, if a sentence is negative, then having "police said" at the beginning will make the statement less severe, and we expect the participants to rate Jay more positively. Similarly, if a sentence is positive, having "police said" at the end will weaken the positive statement less, and we expect the participants to rate Jay more positively. Both of these situations represent the bias=positiveBias condition. On the other hand, placing "police said" at the end of a negative sentence weakens the sentence less, and we expect people to rate Jay more negatively. Similarly, placing "police said" at the beginning of a positive sentence weakens the sentence more, and we expect people to rate Jay more negatively. Both of these situations represent the bias=negativeBias condition.

#Analyze the data to try to determine which of the variables discussed (and their interactions) predict participants' favorability ratings of the people in the stories. In addition, we are also interested in age and gender, although you don't need to include these as interactions with other variables.

#Note: the data in columns "situation" and "person" were used to determine whether the participant was paying attention. If they were, there should be a value of "burglary" or "PSA" in the "situation" column, and "WJ" in the person column. All other data is not valid and should be excluded.

clause<- read.table(file.choose(), sep="\t", comment.char="", quote="", header=T)
clause_edit <- subset(clause, clause$person=="WJ" & (clause$situation=="burglary" | clause$situation=="PSA"))

clause_edit2<- na.omit(clause_edit)

clause_order_model <- lm(normV~Exp+storyType+bias+Exp: storyType + Exp:bias + storyType:bias + age+sex, data=clause_edit2)
summary(clause_order_model)
drop1(clause_order_model, test="F")
# We can drop the interaction bias:storyType and Exp:Bias, because it has a lower AIC and is highly non-significant.
clause_order_model2 <- lm(normV~Exp+storyType+bias+Exp: storyType + age+sex, data=clause_edit2)

summary(clause_order_model2)

#Checking Assumptions:
#Auto-correlation
durbinWatsonTest(clause_order_model2)
#Statistic= 2.170837 , quite close to 2, so assumption has proably been met, so probably not correlated. P-value of Value .14 confirms this, it is bigger than .05, and therefore non-significant. The value however is more than 2, so indicates a negative correlation. So this assumption has been met, and thus the residuals are uncorrelated. This doesn't indicates some amount of autocorrelation.

#Homoscedasticity normality, and high influence points

#When checking for multicollinearity, note that vif values for interactions and single predictors that participate in interactions can be quite high. This is nothing to worry about. Single predictors that do not participate in interactions, however, should have vif values below 10.

vif(clause_order_model2)          
#No concering VIF values. All around 1.

#Residuals:
par(mfrow=c(2,2))
par(mar=c(4,4,4,4))
plot (clause_order_model2)

#Linearity: There is a sort of curve in the (first) graph, but it isn't that big of a curve, so this probably does indicate that the data hasn't violated the assumption of linearity.

#Normal distributed: Some of the datapoints don't fall kinda nicely on the line, but it seems pretty normal. And we have way more than 30 datapoints, so we can assume it's normal.
#Homoscedasticity: this assumption doesn't hold, the datapoints do really funnel out. Also it doesn't really looks like a random array of dots evenly dispersed around zero. So this does indicate variance across residuals and thus does indicate heteroscedasticity. 

#Leverage points:
dim(clause_edit)
averageclause<- (1+13)/297; averageclause #=  0.01 #(k+1)/n

clauseHat <- hatvalues(clause_order_model2) 
clauseTable<-clauseHat > 2 *  averageclause | clauseHat > 3*   averageclause; clauseTable
table(clauseTable)
#None of the values bigger than the ave. value, and thus none of the datapoints affect the data points positioning.
#Also if you look at the graph: we don't have datapoints with extreme x-values, so we don't have leverage points, so we don't have cause for concern. 

#Influential points:
cookclause<-cooks.distance(clause_order_model2)
cookclauseTable<-cookclause > 1; cookclauseTable
table(cookclauseTable)
#None of the values are bigger than 1. So, there are no influential points, and thus no outliers, that may affect the slope of the regression line.
#If you also look at the graph non of the points fall in the cook's distance and thus influence the cloud of the dots. The cloud of datapoints is pretty close to each other, and we don't have one point which affects the regression line highly.

plot(allEffects(clause_order_model2))
#Shows:
#That when Exp was pos and storyType was mixed, normV was high and it was very low when Exp was neg and storyType was accordant. It was a bit low when storytype was mixed and exp was neg, and higher when storyType was mixed, and exp was pos.

Anova(clause_order_model2, type="III")
summary(clause_order_model2)

#Reporting: The final modes was: normV~Exp+storyType+bias+Exp: storyType + Exp:bias+ age+sex. Significant main effects were Exp, storyType and sex (p<0.05). However, age was marginally significant. While the only significant interaction being Exp:Storytype.
#The model was highly significant overall (F(6,290)=45,91, p<0.001) and achieved a high variance explanation (Multiple R-squared:  0.4871,	Adjusted R-squared:  0.4765 ). All regression coefficients, as well as their standard errors, t-scores, and p-values, are provided in the appendix, and checking of model assumptions revealed problems with homoscedasticity.

#Appendix:
#  Coefficients:
#  Estimate Std. Error t value Pr(>|t|)    
#(Intercept)           -13.5849     5.0106  -2.711  0.00710 ** 
#  ExpPos                 55.9614     3.6627  15.279  < 2e-16 ***
#  storyTypemixed         14.6510     3.1405   4.665 4.71e-06 ***
#  biaspositiveBias        5.2984     2.5014   2.118  0.03501 *  
#  age                    -0.1986     0.1016  -1.954  0.05169 .  
#sexMale                -7.0485     2.5961  -2.715  0.00702 ** 
#ExpPos:storyTypemixed -37.0445     5.2353  -7.076 1.12e-11 ***

#In addition to the standard reporting of the results, please give a brief discussion of the findings. For example, if there is a significant interaction, talk about why this might be. What does the interaction actually mean logically? It's ok to speculate a bit here, since this is a statistics course and not a course about any one particular field.

# Well the signifiacnt main effects were Exp, StorytYpe and sex. Thus we can conclude that whether a sentence was negative or positive, had an pretty good influence on how favorable people felt about the people out of the stories. Also apparantly the kind of storyType was highly related to how favorable people felt, thus an negative story influenced negative thoughts and a positive story influenced positive thoughts wrt to normV. Also apparantly the gender of a person, man or woman was related to the favorability. The only interaction that was significant was Exp:storyType. Indicating that: when Exp was pos and storyType was mixed, normV was indeed higher and it was indeed very low when Exp was neg and storyType was accordant. It was also indeed a bit lower when storytype was mixed and exp was neg, and higher when storyType was mixed, and exp was pos.
